{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding, GlobalMaxPooling1D\n",
    "from keras.layers import Dense, Input, Flatten, Concatenate, Reshape\n",
    "from keras.layers import Conv1D, MaxPooling1D,MaxPooling2D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading data\n",
    "df = pd.read_csv('combined_russell.csv', encoding='latin')\n",
    "emotions = ['angry','happy','sad','relaxed']\n",
    "df = df[df.Mood.isin(emotions)]\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates(subset='track_id')\n",
    "\n",
    "lyrics = shuffle(df)\n",
    "lyrics = lyrics.reset_index(drop=True)\n",
    "print('Shape of dataset ',df.shape)\n",
    "print(df.columns)\n",
    "print('No. of unique classes',len(set(df['Mood'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset  (156060, 4)\n",
      "Index(['PhraseId', 'SentenceId', 'Phrase', 'Sentiment'], dtype='object')\n",
      "No. of unique classes 5\n"
     ]
    }
   ],
   "source": [
    "# reading data\n",
    "df1 = pd.read_table('train.tsv', encoding='latin')\n",
    "df1 = df1.dropna()\n",
    "sentiment = shuffle(df1)\n",
    "\n",
    "sentiment = lyrics.reset_index(drop=True)\n",
    "print('Shape of dataset ',df1.shape)\n",
    "print(df1.columns)\n",
    "print('No. of unique classes',len(set(df1['Sentiment'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119315</td>\n",
       "      <td>6377</td>\n",
       "      <td>that it 's implosion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105701</td>\n",
       "      <td>5580</td>\n",
       "      <td>If you 're looking for a tale of Brits behavin...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131240</td>\n",
       "      <td>7073</td>\n",
       "      <td>wasting away</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151491</td>\n",
       "      <td>8261</td>\n",
       "      <td>children 's entertainment , superhero comics ,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94584</td>\n",
       "      <td>4937</td>\n",
       "      <td>the script , credited to director Abdul Malik ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    119315        6377                               that it 's implosion   \n",
       "1    105701        5580  If you 're looking for a tale of Brits behavin...   \n",
       "2    131240        7073                                       wasting away   \n",
       "3    151491        8261  children 's entertainment , superhero comics ,...   \n",
       "4     94584        4937  the script , credited to director Abdul Malik ...   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          1  \n",
       "3          3  \n",
       "4          1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 27273, 2: 79582, 3: 32927, 0: 7072, 4: 9206})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(sentiment.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics.drop(columns=['PhraseId','SentenceId'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>Mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'd like to watch you sleep at night To hear y...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lollipop lollipop Oh lolli lolli lolli Lollipo...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unfortunately, we are not licensed to display ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Love, look at the two of us, strangers in many...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When you can walk a mile into my shoe Criticiz...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics   Mood\n",
       "0  I'd like to watch you sleep at night To hear y...  happy\n",
       "1  Lollipop lollipop Oh lolli lolli lolli Lollipo...  happy\n",
       "2  Unfortunately, we are not licensed to display ...  happy\n",
       "3  Love, look at the two of us, strangers in many...  happy\n",
       "4  When you can walk a mile into my shoe Criticiz...  happy"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = lyrics\n",
    "df = df[['lyrics','Mood']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_timbre1</th>\n",
       "      <th>avg_timbre2</th>\n",
       "      <th>avg_timbre3</th>\n",
       "      <th>avg_timbre4</th>\n",
       "      <th>avg_timbre5</th>\n",
       "      <th>avg_timbre6</th>\n",
       "      <th>avg_timbre7</th>\n",
       "      <th>avg_timbre8</th>\n",
       "      <th>avg_timbre9</th>\n",
       "      <th>avg_timbre10</th>\n",
       "      <th>avg_timbre11</th>\n",
       "      <th>avg_timbre12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.409136</td>\n",
       "      <td>-13.601859</td>\n",
       "      <td>-10.378125</td>\n",
       "      <td>-8.786040</td>\n",
       "      <td>1.038614</td>\n",
       "      <td>-7.920109</td>\n",
       "      <td>6.843805</td>\n",
       "      <td>-6.608333</td>\n",
       "      <td>11.529526</td>\n",
       "      <td>-1.366845</td>\n",
       "      <td>-1.952027</td>\n",
       "      <td>2.335361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.181391</td>\n",
       "      <td>-25.857859</td>\n",
       "      <td>115.477534</td>\n",
       "      <td>38.219435</td>\n",
       "      <td>-7.956962</td>\n",
       "      <td>6.824071</td>\n",
       "      <td>8.513007</td>\n",
       "      <td>2.362920</td>\n",
       "      <td>29.307108</td>\n",
       "      <td>8.674287</td>\n",
       "      <td>2.591052</td>\n",
       "      <td>13.292798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.372331</td>\n",
       "      <td>6.856258</td>\n",
       "      <td>19.296324</td>\n",
       "      <td>59.003976</td>\n",
       "      <td>4.851365</td>\n",
       "      <td>-9.250714</td>\n",
       "      <td>23.003287</td>\n",
       "      <td>-6.075135</td>\n",
       "      <td>-3.033486</td>\n",
       "      <td>10.028944</td>\n",
       "      <td>8.010275</td>\n",
       "      <td>4.652257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.943157</td>\n",
       "      <td>-63.909238</td>\n",
       "      <td>18.113154</td>\n",
       "      <td>-19.529068</td>\n",
       "      <td>2.724330</td>\n",
       "      <td>-15.444521</td>\n",
       "      <td>-16.254649</td>\n",
       "      <td>-8.609178</td>\n",
       "      <td>6.078134</td>\n",
       "      <td>-7.437416</td>\n",
       "      <td>3.291599</td>\n",
       "      <td>14.913005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.630497</td>\n",
       "      <td>5.410468</td>\n",
       "      <td>47.499126</td>\n",
       "      <td>0.893643</td>\n",
       "      <td>-32.775099</td>\n",
       "      <td>-6.375195</td>\n",
       "      <td>-7.997485</td>\n",
       "      <td>1.262054</td>\n",
       "      <td>16.002920</td>\n",
       "      <td>6.764081</td>\n",
       "      <td>1.058027</td>\n",
       "      <td>17.718247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_timbre1  avg_timbre2  avg_timbre3  avg_timbre4  avg_timbre5  \\\n",
       "0    43.409136   -13.601859   -10.378125    -8.786040     1.038614   \n",
       "1    37.181391   -25.857859   115.477534    38.219435    -7.956962   \n",
       "2    44.372331     6.856258    19.296324    59.003976     4.851365   \n",
       "3    42.943157   -63.909238    18.113154   -19.529068     2.724330   \n",
       "4    40.630497     5.410468    47.499126     0.893643   -32.775099   \n",
       "\n",
       "   avg_timbre6  avg_timbre7  avg_timbre8  avg_timbre9  avg_timbre10  \\\n",
       "0    -7.920109     6.843805    -6.608333    11.529526     -1.366845   \n",
       "1     6.824071     8.513007     2.362920    29.307108      8.674287   \n",
       "2    -9.250714    23.003287    -6.075135    -3.033486     10.028944   \n",
       "3   -15.444521   -16.254649    -8.609178     6.078134     -7.437416   \n",
       "4    -6.375195    -7.997485     1.262054    16.002920      6.764081   \n",
       "\n",
       "   avg_timbre11  avg_timbre12  \n",
       "0     -1.952027      2.335361  \n",
       "1      2.591052     13.292798  \n",
       "2      8.010275      4.652257  \n",
       "3      3.291599     14.913005  \n",
       "4      1.058027     17.718247  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = lyrics[['avg_timbre1',\n",
    "       'avg_timbre2', 'avg_timbre3', 'avg_timbre4', 'avg_timbre5',\n",
    "       'avg_timbre6', 'avg_timbre7', 'avg_timbre8', 'avg_timbre9',\n",
    "       'avg_timbre10', 'avg_timbre11', 'avg_timbre12']]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/revanth/tf/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "macronum=sorted(set(df['Mood']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "df['Mood']=df['Mood'].apply(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>Mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'd like to watch you sleep at night To hear y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lollipop lollipop Oh lolli lolli lolli Lollipo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unfortunately, we are not licensed to display ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Love, look at the two of us, strangers in many...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When you can walk a mile into my shoe Criticiz...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  Mood\n",
       "0  I'd like to watch you sleep at night To hear y...     0\n",
       "1  Lollipop lollipop Oh lolli lolli lolli Lollipo...     0\n",
       "2  Unfortunately, we are not licensed to display ...     0\n",
       "3  Love, look at the two of us, strangers in many...     0\n",
       "4  When you can walk a mile into my shoe Criticiz...     0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for idx in range(df.lyrics.shape[0]):\n",
    "    text = BeautifulSoup(df.lyrics[idx])\n",
    "    texts.append(clean_str(str(text.get_text().encode())))\n",
    "\n",
    "for idx in df['Mood']:\n",
    "    labels.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 2823\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Data Tensor: (150, 1000)\n",
      "Shape of Label Tensor: (150, 3)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of Data Tensor:', data.shape)\n",
    "print('Shape of Label Tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.00290095 0.52899069 0.03319502]\n",
      " [0.         0.         0.         ... 0.02818069 0.11811024 0.02753678]\n",
      " [0.         0.         0.         ... 0.00538748 0.07015032 0.02753678]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.04973063 0.00501074 0.04488872]\n",
      " [0.         0.         0.         ... 0.09407377 0.26485326 0.43191249]\n",
      " [0.         0.         0.         ... 0.00124327 1.         0.07355715]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/revanth/tf/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/revanth/tf/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "print(x_train_scaled)\n",
    "scaler = MinMaxScaler()\n",
    "x_val_scaled = scaler.fit_transform(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items(): \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99067109,  0.96596054,  0.72638522, ...,  0.72325509,\n",
       "         0.91466578,  0.63872926],\n",
       "       [-0.49886   ,  0.76602   ,  0.89750999, ..., -0.41179001,\n",
       "         0.40538999,  0.78504002],\n",
       "       [-0.046539  ,  0.61966002,  0.56647003, ..., -0.37616   ,\n",
       "        -0.032502  ,  0.80620003],\n",
       "       ...,\n",
       "       [-0.31790999, -0.15471999, -0.012228  , ...,  0.10023   ,\n",
       "         0.33579999, -0.01561   ],\n",
       "       [ 0.38521001,  0.099276  ,  0.81708997, ..., -0.55181003,\n",
       "         0.65854001, -0.43109   ],\n",
       "       [ 0.28981   ,  0.24276   , -0.37538001, ..., -0.60126001,\n",
       "        -1.17910004,  0.47171   ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(embedding_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "audio_features = scaler.fit_transform(np.array(df2.iloc[:, :-1], dtype = float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_train = audio_features[:-nb_validation_samples]\n",
    "aud_val = audio_features[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1000, 100)    959700      input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 998, 64)      19264       embedding_3[4][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 996, 64)      32064       embedding_3[4][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 991, 64)      64064       embedding_3[4][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_37 (Global (None, 64)           0           conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_38 (Global (None, 64)           0           conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_39 (Global (None, 64)           0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "audio_input (InputLayer)        (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 203)          0           global_max_pooling1d_37[0][0]    \n",
      "                                                                 global_max_pooling1d_38[0][0]    \n",
      "                                                                 global_max_pooling1d_39[0][0]    \n",
      "                                                                 audio_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 64)           13056       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 3)            195         dense_25[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,088,343\n",
      "Trainable params: 1,088,343\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model to fit audio and lyrics\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "audio_input = Input(shape=(11,), name = 'audio_input')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(64, 3, activation='relu')(embedded_sequences)\n",
    "l_pool1 = GlobalMaxPooling1D()(l_cov1)\n",
    "l_cov2 = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "l_pool2 = GlobalMaxPooling1D()(l_cov2)\n",
    "l_cov3 = Conv1D(64, 10, activation='relu')(embedded_sequences)\n",
    "l_pool3 = GlobalMaxPooling1D()(l_cov3)  # global max pooling\n",
    "l_flat = Concatenate()([l_pool1,l_pool2,l_pool3,audio_input])\n",
    "l_dense = Dense(64, activation='sigmoid')(l_flat)\n",
    "preds = Dense(len(macronum), activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(inputs = [sequence_input, audio_input], outputs = preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn_128_long.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1032 samples, validate on 258 samples\n",
      "Epoch 1/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 1.1098 - acc: 0.3953 - val_loss: 1.1164 - val_acc: 0.3760\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.37597, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 2/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.9507 - acc: 0.5833 - val_loss: 1.2159 - val_acc: 0.3411\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.37597\n",
      "Epoch 3/200\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.8503 - acc: 0.6357 - val_loss: 1.0256 - val_acc: 0.4457\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.37597 to 0.44574, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 4/200\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.7090 - acc: 0.7626 - val_loss: 0.9284 - val_acc: 0.5349\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.44574 to 0.53488, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 5/200\n",
      "1032/1032 [==============================] - 37s 35ms/step - loss: 0.6206 - acc: 0.7829 - val_loss: 0.8616 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.53488 to 0.58140, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 6/200\n",
      "1032/1032 [==============================] - 42s 41ms/step - loss: 0.4764 - acc: 0.9041 - val_loss: 1.3339 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.58140\n",
      "Epoch 7/200\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 0.4307 - acc: 0.8963 - val_loss: 0.8397 - val_acc: 0.6085\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.58140 to 0.60853, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 8/200\n",
      "1032/1032 [==============================] - 43s 42ms/step - loss: 0.3283 - acc: 0.9205 - val_loss: 0.9080 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.60853\n",
      "Epoch 9/200\n",
      "1032/1032 [==============================] - 38s 37ms/step - loss: 0.2789 - acc: 0.9302 - val_loss: 1.1374 - val_acc: 0.5233\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.60853\n",
      "Epoch 10/200\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.2382 - acc: 0.9409 - val_loss: 0.8546 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.60853\n",
      "Epoch 11/200\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.1735 - acc: 0.9603 - val_loss: 1.3096 - val_acc: 0.5465\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.60853\n",
      "Epoch 12/200\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.1595 - acc: 0.9554 - val_loss: 0.9205 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.60853\n",
      "Epoch 13/200\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.1316 - acc: 0.9680 - val_loss: 1.3523 - val_acc: 0.5581\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.60853\n",
      "Epoch 14/200\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.1247 - acc: 0.9564 - val_loss: 0.9600 - val_acc: 0.6085\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.60853\n",
      "Epoch 15/200\n",
      "1032/1032 [==============================] - 39s 37ms/step - loss: 0.1373 - acc: 0.9554 - val_loss: 0.9524 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.60853\n",
      "Epoch 16/200\n",
      "1032/1032 [==============================] - 38s 36ms/step - loss: 0.0953 - acc: 0.9690 - val_loss: 1.2902 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.60853\n",
      "Epoch 17/200\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.1002 - acc: 0.9641 - val_loss: 1.0379 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.60853\n",
      "Epoch 18/200\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0946 - acc: 0.9651 - val_loss: 1.0511 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.60853\n",
      "Epoch 19/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0982 - acc: 0.9622 - val_loss: 1.3037 - val_acc: 0.5543\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.60853\n",
      "Epoch 20/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0857 - acc: 0.9700 - val_loss: 2.1297 - val_acc: 0.4806\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.60853\n",
      "Epoch 21/200\n",
      "1032/1032 [==============================] - 40s 38ms/step - loss: 0.1016 - acc: 0.9593 - val_loss: 1.2383 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.60853\n",
      "Epoch 22/200\n",
      "1032/1032 [==============================] - 42s 41ms/step - loss: 0.0864 - acc: 0.9651 - val_loss: 1.0697 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.60853\n",
      "Epoch 23/200\n",
      "1032/1032 [==============================] - 42s 41ms/step - loss: 0.0721 - acc: 0.9709 - val_loss: 1.0907 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.60853\n",
      "Epoch 24/200\n",
      "1032/1032 [==============================] - 38s 37ms/step - loss: 0.0879 - acc: 0.9690 - val_loss: 1.1031 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.60853\n",
      "Epoch 25/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0645 - acc: 0.9719 - val_loss: 1.1186 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.60853\n",
      "Epoch 26/200\n",
      "1032/1032 [==============================] - 38s 37ms/step - loss: 0.0752 - acc: 0.9690 - val_loss: 1.1241 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.60853\n",
      "Epoch 27/200\n",
      "1032/1032 [==============================] - 38s 37ms/step - loss: 0.0737 - acc: 0.9690 - val_loss: 1.2626 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.60853\n",
      "Epoch 28/200\n",
      "1032/1032 [==============================] - 38s 37ms/step - loss: 0.0644 - acc: 0.9738 - val_loss: 1.2286 - val_acc: 0.6124\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.60853 to 0.61240, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 29/200\n",
      "1032/1032 [==============================] - 39s 37ms/step - loss: 0.0609 - acc: 0.9748 - val_loss: 1.3322 - val_acc: 0.6085\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.61240\n",
      "Epoch 30/200\n",
      "1032/1032 [==============================] - 39s 37ms/step - loss: 0.0640 - acc: 0.9748 - val_loss: 1.1980 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.61240\n",
      "Epoch 31/200\n",
      "1032/1032 [==============================] - 40s 38ms/step - loss: 0.0627 - acc: 0.9709 - val_loss: 1.2639 - val_acc: 0.6202\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.61240 to 0.62016, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 32/200\n",
      "1032/1032 [==============================] - 39s 37ms/step - loss: 0.0642 - acc: 0.9719 - val_loss: 1.2197 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.62016\n",
      "Epoch 33/200\n",
      "1032/1032 [==============================] - 38s 37ms/step - loss: 0.0622 - acc: 0.9690 - val_loss: 1.2670 - val_acc: 0.6085\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.62016\n",
      "Epoch 34/200\n",
      "1032/1032 [==============================] - 38s 37ms/step - loss: 0.0559 - acc: 0.9738 - val_loss: 1.2677 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.62016\n",
      "Epoch 35/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0580 - acc: 0.9719 - val_loss: 1.3027 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.62016\n",
      "Epoch 36/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0591 - acc: 0.9719 - val_loss: 1.3855 - val_acc: 0.5620\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.62016\n",
      "Epoch 37/200\n",
      "1032/1032 [==============================] - 42s 41ms/step - loss: 0.0548 - acc: 0.9738 - val_loss: 1.4736 - val_acc: 0.5659\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.62016\n",
      "Epoch 38/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0561 - acc: 0.9729 - val_loss: 1.5350 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.62016\n",
      "Epoch 39/200\n",
      "1032/1032 [==============================] - 39s 37ms/step - loss: 0.0523 - acc: 0.9748 - val_loss: 1.3548 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.62016\n",
      "Epoch 40/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0556 - acc: 0.9709 - val_loss: 1.5208 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.62016\n",
      "Epoch 41/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0550 - acc: 0.9738 - val_loss: 1.6215 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.62016\n",
      "Epoch 42/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0549 - acc: 0.9709 - val_loss: 1.4757 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.62016\n",
      "Epoch 43/200\n",
      "1032/1032 [==============================] - 38s 37ms/step - loss: 0.0529 - acc: 0.9758 - val_loss: 1.3747 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.62016\n",
      "Epoch 44/200\n",
      "1032/1032 [==============================] - 41s 40ms/step - loss: 0.0523 - acc: 0.9719 - val_loss: 2.0315 - val_acc: 0.5543\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.62016\n",
      "Epoch 45/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0522 - acc: 0.9738 - val_loss: 1.4850 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.62016\n",
      "Epoch 46/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0516 - acc: 0.9748 - val_loss: 1.8882 - val_acc: 0.5581\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.62016\n",
      "Epoch 47/200\n",
      "1032/1032 [==============================] - 46s 45ms/step - loss: 0.0509 - acc: 0.9767 - val_loss: 1.4914 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.62016\n",
      "Epoch 48/200\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 0.0507 - acc: 0.9738 - val_loss: 1.5024 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.62016\n",
      "Epoch 49/200\n",
      "1032/1032 [==============================] - 42s 40ms/step - loss: 0.0503 - acc: 0.9729 - val_loss: 1.5017 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.62016\n",
      "Epoch 50/200\n",
      "1032/1032 [==============================] - 44s 42ms/step - loss: 0.0495 - acc: 0.9748 - val_loss: 1.5576 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.62016\n",
      "Epoch 51/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0491 - acc: 0.9758 - val_loss: 1.5050 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.62016\n",
      "Epoch 52/200\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 0.0501 - acc: 0.9738 - val_loss: 1.5185 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.62016\n",
      "Epoch 53/200\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 0.0498 - acc: 0.9729 - val_loss: 1.6212 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.62016\n",
      "Epoch 54/200\n",
      "1032/1032 [==============================] - 42s 41ms/step - loss: 0.0496 - acc: 0.9767 - val_loss: 1.7069 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.62016\n",
      "Epoch 55/200\n",
      "1032/1032 [==============================] - 43s 42ms/step - loss: 0.0467 - acc: 0.9777 - val_loss: 1.6673 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.62016\n",
      "Epoch 56/200\n",
      "1032/1032 [==============================] - 55s 54ms/step - loss: 0.0488 - acc: 0.9748 - val_loss: 2.0516 - val_acc: 0.5620\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.62016\n",
      "Epoch 57/200\n",
      "1032/1032 [==============================] - 57s 55ms/step - loss: 0.0505 - acc: 0.9758 - val_loss: 1.6591 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.62016\n",
      "Epoch 58/200\n",
      "1032/1032 [==============================] - 51s 49ms/step - loss: 0.0478 - acc: 0.9758 - val_loss: 1.7868 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.62016\n",
      "Epoch 59/200\n",
      "1032/1032 [==============================] - 48s 46ms/step - loss: 0.0494 - acc: 0.9748 - val_loss: 1.7579 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.62016\n",
      "Epoch 60/200\n",
      "1032/1032 [==============================] - 50s 49ms/step - loss: 0.0498 - acc: 0.9758 - val_loss: 1.6752 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.62016\n",
      "Epoch 61/200\n",
      "1032/1032 [==============================] - 46s 44ms/step - loss: 0.0481 - acc: 0.9748 - val_loss: 1.7124 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.62016\n",
      "Epoch 62/200\n",
      "1032/1032 [==============================] - 44s 42ms/step - loss: 0.0488 - acc: 0.9719 - val_loss: 1.7270 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.62016\n",
      "Epoch 63/200\n",
      "1032/1032 [==============================] - 41s 39ms/step - loss: 0.0485 - acc: 0.9748 - val_loss: 1.6566 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.62016\n",
      "Epoch 64/200\n",
      "1032/1032 [==============================] - 54s 52ms/step - loss: 0.0487 - acc: 0.9748 - val_loss: 1.7447 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.62016\n",
      "Epoch 65/200\n",
      "1032/1032 [==============================] - 46s 45ms/step - loss: 0.0486 - acc: 0.9758 - val_loss: 1.8032 - val_acc: 0.6085\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.62016\n",
      "Epoch 66/200\n",
      "1032/1032 [==============================] - 42s 41ms/step - loss: 0.0485 - acc: 0.9777 - val_loss: 1.8856 - val_acc: 0.6202\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.62016\n",
      "Epoch 67/200\n",
      "1032/1032 [==============================] - 45s 43ms/step - loss: 0.0467 - acc: 0.9777 - val_loss: 1.7418 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.62016\n",
      "Epoch 68/200\n",
      "1032/1032 [==============================] - 47s 45ms/step - loss: 0.0484 - acc: 0.9738 - val_loss: 1.8142 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.62016\n",
      "Epoch 69/200\n",
      "1032/1032 [==============================] - 48s 46ms/step - loss: 0.0469 - acc: 0.9777 - val_loss: 1.8093 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.62016\n",
      "Epoch 70/200\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 0.0466 - acc: 0.9758 - val_loss: 1.7997 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.62016\n",
      "Epoch 71/200\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 0.0470 - acc: 0.9767 - val_loss: 1.7649 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.62016\n",
      "Epoch 72/200\n",
      "1032/1032 [==============================] - 41s 39ms/step - loss: 0.0469 - acc: 0.9758 - val_loss: 1.8097 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.62016\n",
      "Epoch 73/200\n",
      "1032/1032 [==============================] - 41s 39ms/step - loss: 0.0464 - acc: 0.9787 - val_loss: 1.9140 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.62016\n",
      "Epoch 74/200\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 0.0453 - acc: 0.9767 - val_loss: 1.8495 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.62016\n",
      "Epoch 75/200\n",
      "1032/1032 [==============================] - 40s 38ms/step - loss: 0.0450 - acc: 0.9777 - val_loss: 2.0605 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.62016\n",
      "Epoch 76/200\n",
      "1032/1032 [==============================] - 47s 46ms/step - loss: 0.0461 - acc: 0.9738 - val_loss: 1.8756 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.62016\n",
      "Epoch 77/200\n",
      "1032/1032 [==============================] - 37s 35ms/step - loss: 0.0465 - acc: 0.9787 - val_loss: 1.8376 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.62016\n",
      "Epoch 78/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0466 - acc: 0.9758 - val_loss: 1.8267 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.62016\n",
      "Epoch 79/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0444 - acc: 0.9777 - val_loss: 1.8908 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.62016\n",
      "Epoch 80/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0451 - acc: 0.9767 - val_loss: 1.9939 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.62016\n",
      "Epoch 81/200\n",
      "1032/1032 [==============================] - 37s 35ms/step - loss: 0.0461 - acc: 0.9738 - val_loss: 1.9331 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.62016\n",
      "Epoch 82/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0458 - acc: 0.9777 - val_loss: 1.9498 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.62016\n",
      "Epoch 83/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0449 - acc: 0.9767 - val_loss: 1.8615 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.62016\n",
      "Epoch 84/200\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0448 - acc: 0.9787 - val_loss: 1.8784 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.62016\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0460 - acc: 0.9738 - val_loss: 2.0541 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.62016\n",
      "Epoch 86/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0443 - acc: 0.9787 - val_loss: 2.1087 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.62016\n",
      "Epoch 87/200\n",
      "1032/1032 [==============================] - 38s 36ms/step - loss: 0.0445 - acc: 0.9777 - val_loss: 2.0395 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.62016\n",
      "Epoch 88/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0435 - acc: 0.9767 - val_loss: 1.9983 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.62016\n",
      "Epoch 89/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0442 - acc: 0.9777 - val_loss: 2.0571 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.62016\n",
      "Epoch 90/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0447 - acc: 0.9758 - val_loss: 1.9939 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.62016\n",
      "Epoch 91/200\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0440 - acc: 0.9767 - val_loss: 2.1621 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.62016\n",
      "Epoch 92/200\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0437 - acc: 0.9748 - val_loss: 2.0922 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.62016\n",
      "Epoch 93/200\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0436 - acc: 0.9767 - val_loss: 2.0868 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.62016\n",
      "Epoch 94/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0433 - acc: 0.9777 - val_loss: 2.3115 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.62016\n",
      "Epoch 95/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0431 - acc: 0.9777 - val_loss: 2.1256 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.62016\n",
      "Epoch 96/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0428 - acc: 0.9777 - val_loss: 2.0211 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.62016\n",
      "Epoch 97/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0424 - acc: 0.9777 - val_loss: 2.1206 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.62016\n",
      "Epoch 98/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0425 - acc: 0.9758 - val_loss: 1.9918 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.62016\n",
      "Epoch 99/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0424 - acc: 0.9806 - val_loss: 2.1402 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.62016\n",
      "Epoch 100/200\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0431 - acc: 0.9777 - val_loss: 2.0938 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.62016\n",
      "Epoch 101/200\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0424 - acc: 0.9787 - val_loss: 2.1272 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.62016\n",
      "Epoch 102/200\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0420 - acc: 0.9787 - val_loss: 2.2945 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.62016\n",
      "Epoch 103/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0432 - acc: 0.9758 - val_loss: 2.1305 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.62016\n",
      "Epoch 104/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0420 - acc: 0.9797 - val_loss: 2.0159 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.62016\n",
      "Epoch 105/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0424 - acc: 0.9797 - val_loss: 2.0892 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.62016\n",
      "Epoch 106/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0417 - acc: 0.9777 - val_loss: 2.2921 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.62016\n",
      "Epoch 107/200\n",
      "1032/1032 [==============================] - 37s 35ms/step - loss: 0.0418 - acc: 0.9777 - val_loss: 2.1844 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.62016\n",
      "Epoch 108/200\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0410 - acc: 0.9845 - val_loss: 2.1156 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.62016\n",
      "Epoch 109/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0420 - acc: 0.9797 - val_loss: 2.1283 - val_acc: 0.6124\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.62016\n",
      "Epoch 110/200\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0413 - acc: 0.9797 - val_loss: 2.1365 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.62016\n",
      "Epoch 111/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0414 - acc: 0.9797 - val_loss: 2.1386 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.62016\n",
      "Epoch 112/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0402 - acc: 0.9826 - val_loss: 2.0769 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.62016\n",
      "Epoch 113/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0410 - acc: 0.9806 - val_loss: 2.0823 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.62016\n",
      "Epoch 114/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0405 - acc: 0.9835 - val_loss: 2.0791 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.62016\n",
      "Epoch 115/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0406 - acc: 0.9787 - val_loss: 2.3268 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.62016\n",
      "Epoch 116/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0409 - acc: 0.9816 - val_loss: 2.1717 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.62016\n",
      "Epoch 117/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0400 - acc: 0.9826 - val_loss: 2.2257 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.62016\n",
      "Epoch 118/200\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0395 - acc: 0.9816 - val_loss: 2.5861 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.62016\n",
      "Epoch 119/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0412 - acc: 0.9787 - val_loss: 2.2965 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.62016\n",
      "Epoch 120/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0404 - acc: 0.9806 - val_loss: 2.1436 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.62016\n",
      "Epoch 121/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0399 - acc: 0.9816 - val_loss: 2.2393 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.62016\n",
      "Epoch 122/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0399 - acc: 0.9816 - val_loss: 2.4400 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.62016\n",
      "Epoch 123/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0398 - acc: 0.9787 - val_loss: 2.1842 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.62016\n",
      "Epoch 124/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0398 - acc: 0.9797 - val_loss: 2.2418 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.62016\n",
      "Epoch 125/200\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0397 - acc: 0.9835 - val_loss: 2.2335 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.62016\n",
      "Epoch 126/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0396 - acc: 0.9845 - val_loss: 2.3272 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.62016\n",
      "Epoch 127/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0395 - acc: 0.9797 - val_loss: 2.3822 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.62016\n",
      "Epoch 128/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0397 - acc: 0.9835 - val_loss: 2.2540 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.62016\n",
      "Epoch 129/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0393 - acc: 0.9797 - val_loss: 2.2292 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.62016\n",
      "Epoch 130/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0399 - acc: 0.9835 - val_loss: 2.3693 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.62016\n",
      "Epoch 131/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0392 - acc: 0.9806 - val_loss: 2.2422 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.62016\n",
      "Epoch 132/200\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0393 - acc: 0.9835 - val_loss: 2.2683 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.62016\n",
      "Epoch 133/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0400 - acc: 0.9806 - val_loss: 2.2340 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.62016\n",
      "Epoch 134/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0401 - acc: 0.9797 - val_loss: 2.4346 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.62016\n",
      "Epoch 135/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0406 - acc: 0.9835 - val_loss: 2.2503 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.62016\n",
      "Epoch 136/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0413 - acc: 0.9816 - val_loss: 2.4215 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.62016\n",
      "Epoch 137/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0393 - acc: 0.9816 - val_loss: 2.3902 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.62016\n",
      "Epoch 138/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0389 - acc: 0.9835 - val_loss: 2.5419 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.62016\n",
      "Epoch 139/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0438 - acc: 0.9797 - val_loss: 2.3302 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.62016\n",
      "Epoch 140/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0404 - acc: 0.9787 - val_loss: 2.3921 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.62016\n",
      "Epoch 141/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0380 - acc: 0.9797 - val_loss: 2.3443 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.62016\n",
      "Epoch 142/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0398 - acc: 0.9835 - val_loss: 2.3606 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.62016\n",
      "Epoch 143/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0384 - acc: 0.9806 - val_loss: 2.4954 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.62016\n",
      "Epoch 144/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0370 - acc: 0.9826 - val_loss: 2.5612 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.62016\n",
      "Epoch 145/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0386 - acc: 0.9816 - val_loss: 2.3416 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.62016\n",
      "Epoch 146/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0375 - acc: 0.9826 - val_loss: 2.4535 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.62016\n",
      "Epoch 147/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0376 - acc: 0.9826 - val_loss: 2.3933 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.62016\n",
      "Epoch 148/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0390 - acc: 0.9845 - val_loss: 2.3748 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.62016\n",
      "Epoch 149/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0382 - acc: 0.9826 - val_loss: 2.3781 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.62016\n",
      "Epoch 150/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0385 - acc: 0.9806 - val_loss: 2.5200 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.62016\n",
      "Epoch 151/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0388 - acc: 0.9826 - val_loss: 2.3634 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.62016\n",
      "Epoch 152/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0382 - acc: 0.9835 - val_loss: 2.4987 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.62016\n",
      "Epoch 153/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0375 - acc: 0.9816 - val_loss: 2.3786 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.62016\n",
      "Epoch 154/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0370 - acc: 0.9816 - val_loss: 2.3874 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.62016\n",
      "Epoch 155/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0373 - acc: 0.9855 - val_loss: 2.4767 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.62016\n",
      "Epoch 156/200\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0381 - acc: 0.9845 - val_loss: 2.4383 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.62016\n",
      "Epoch 157/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0370 - acc: 0.9835 - val_loss: 2.5599 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.62016\n",
      "Epoch 158/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0367 - acc: 0.9864 - val_loss: 2.6585 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.62016\n",
      "Epoch 159/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0374 - acc: 0.9797 - val_loss: 2.4296 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.62016\n",
      "Epoch 160/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0360 - acc: 0.9855 - val_loss: 2.4367 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.62016\n",
      "Epoch 161/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0360 - acc: 0.9835 - val_loss: 2.4132 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.62016\n",
      "Epoch 162/200\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0354 - acc: 0.9855 - val_loss: 2.5089 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.62016\n",
      "Epoch 163/200\n",
      "1032/1032 [==============================] - 42s 40ms/step - loss: 0.0348 - acc: 0.9855 - val_loss: 2.6902 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.62016\n",
      "Epoch 164/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0365 - acc: 0.9835 - val_loss: 2.5212 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.62016\n",
      "Epoch 165/200\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0350 - acc: 0.9835 - val_loss: 2.5440 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.62016\n",
      "Epoch 166/200\n",
      "1032/1032 [==============================] - 37s 35ms/step - loss: 0.0335 - acc: 0.9864 - val_loss: 2.5378 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.62016\n",
      "Epoch 167/200\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0381 - acc: 0.9826 - val_loss: 2.5182 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.62016\n",
      "Epoch 168/200\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 0.0343 - acc: 0.9864 - val_loss: 2.5148 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.62016\n",
      "Epoch 169/200\n",
      "1032/1032 [==============================] - 43s 41ms/step - loss: 0.0361 - acc: 0.9826 - val_loss: 2.5177 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.62016\n",
      "Epoch 170/200\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 0.0335 - acc: 0.9845 - val_loss: 2.5130 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.62016\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0346 - acc: 0.9816 - val_loss: 2.5360 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.62016\n",
      "Epoch 172/200\n",
      "1032/1032 [==============================] - 37s 35ms/step - loss: 0.0373 - acc: 0.9816 - val_loss: 2.5246 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.62016\n",
      "Epoch 173/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0338 - acc: 0.9855 - val_loss: 2.6429 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.62016\n",
      "Epoch 174/200\n",
      "1032/1032 [==============================] - 44s 43ms/step - loss: 0.0339 - acc: 0.9874 - val_loss: 2.6005 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.62016\n",
      "Epoch 175/200\n",
      "1032/1032 [==============================] - 56s 54ms/step - loss: 0.0348 - acc: 0.9855 - val_loss: 2.5798 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.62016\n",
      "Epoch 176/200\n",
      "1032/1032 [==============================] - 50s 48ms/step - loss: 0.0341 - acc: 0.9806 - val_loss: 2.6013 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.62016\n",
      "Epoch 177/200\n",
      "1032/1032 [==============================] - 46s 45ms/step - loss: 0.0361 - acc: 0.9845 - val_loss: 2.5933 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.62016\n",
      "Epoch 178/200\n",
      "1032/1032 [==============================] - 44s 42ms/step - loss: 0.0344 - acc: 0.9855 - val_loss: 2.6075 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.62016\n",
      "Epoch 179/200\n",
      "1032/1032 [==============================] - 43s 41ms/step - loss: 0.0341 - acc: 0.9835 - val_loss: 2.5836 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.62016\n",
      "Epoch 180/200\n",
      "1032/1032 [==============================] - 44s 43ms/step - loss: 0.0340 - acc: 0.9835 - val_loss: 2.6179 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.62016\n",
      "Epoch 181/200\n",
      "1032/1032 [==============================] - 43s 42ms/step - loss: 0.0345 - acc: 0.9864 - val_loss: 2.7957 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.62016\n",
      "Epoch 182/200\n",
      "1032/1032 [==============================] - 44s 42ms/step - loss: 0.0333 - acc: 0.9884 - val_loss: 2.5992 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.62016\n",
      "Epoch 183/200\n",
      "1032/1032 [==============================] - 46s 44ms/step - loss: 0.0324 - acc: 0.9864 - val_loss: 2.5425 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.62016\n",
      "Epoch 184/200\n",
      "1032/1032 [==============================] - 46s 45ms/step - loss: 0.0340 - acc: 0.9845 - val_loss: 2.7071 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.62016\n",
      "Epoch 185/200\n",
      "1032/1032 [==============================] - 55s 53ms/step - loss: 0.0326 - acc: 0.9855 - val_loss: 2.6431 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.62016\n",
      "Epoch 186/200\n",
      "1032/1032 [==============================] - 50s 49ms/step - loss: 0.0327 - acc: 0.9864 - val_loss: 2.5752 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.62016\n",
      "Epoch 187/200\n",
      "1032/1032 [==============================] - 50s 48ms/step - loss: 0.0326 - acc: 0.9826 - val_loss: 2.5932 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.62016\n",
      "Epoch 188/200\n",
      "1032/1032 [==============================] - 51s 50ms/step - loss: 0.0328 - acc: 0.9826 - val_loss: 2.7088 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.62016\n",
      "Epoch 189/200\n",
      "1032/1032 [==============================] - 49s 48ms/step - loss: 0.0325 - acc: 0.9855 - val_loss: 2.6015 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.62016\n",
      "Epoch 190/200\n",
      "1032/1032 [==============================] - 46s 45ms/step - loss: 0.0333 - acc: 0.9845 - val_loss: 2.6695 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.62016\n",
      "Epoch 191/200\n",
      "1032/1032 [==============================] - 46s 45ms/step - loss: 0.0325 - acc: 0.9864 - val_loss: 2.6177 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.62016\n",
      "Epoch 192/200\n",
      "1032/1032 [==============================] - 47s 45ms/step - loss: 0.0324 - acc: 0.9855 - val_loss: 2.6499 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.62016\n",
      "Epoch 193/200\n",
      "1032/1032 [==============================] - 55s 53ms/step - loss: 0.0322 - acc: 0.9864 - val_loss: 2.6573 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.62016\n",
      "Epoch 194/200\n",
      "1032/1032 [==============================] - 49s 48ms/step - loss: 0.0320 - acc: 0.9855 - val_loss: 2.6783 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.62016\n",
      "Epoch 195/200\n",
      "1032/1032 [==============================] - 52s 50ms/step - loss: 0.0319 - acc: 0.9855 - val_loss: 2.6470 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.62016\n",
      "Epoch 196/200\n",
      "1032/1032 [==============================] - 59s 57ms/step - loss: 0.0321 - acc: 0.9845 - val_loss: 2.6695 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.62016\n",
      "Epoch 197/200\n",
      "1032/1032 [==============================] - 49s 48ms/step - loss: 0.0315 - acc: 0.9855 - val_loss: 2.6911 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.62016\n",
      "Epoch 198/200\n",
      "1032/1032 [==============================] - 43s 41ms/step - loss: 0.0315 - acc: 0.9845 - val_loss: 2.6702 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.62016\n",
      "Epoch 199/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0309 - acc: 0.9884 - val_loss: 2.8137 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.62016\n",
      "Epoch 200/200\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0316 - acc: 0.9864 - val_loss: 2.7112 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.62016\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_train, aud_train], y_train, validation_data=([x_val, aud_val], y_val),epochs=200, batch_size=64,callbacks=[cp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1000, 100)    1299400     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 996, 128)     64128       embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 991, 128)     128128      embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 986, 128)     192128      embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 128)          0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 128)          0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 128)          0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 384)          0           global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          49280       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 4)            516         dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,733,580\n",
      "Trainable params: 1,733,580\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = GlobalMaxPooling1D()(l_cov1)\n",
    "l_cov2 = Conv1D(128, 10, activation='relu')(embedded_sequences)\n",
    "l_pool2 = GlobalMaxPooling1D()(l_cov2)\n",
    "l_cov3 = Conv1D(128, 15, activation='relu')(embedded_sequences)\n",
    "l_pool3 = GlobalMaxPooling1D()(l_cov3)  # global max pooling\n",
    "l_flat = Concatenate()([l_pool1,l_pool2,l_pool3])\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(len(macronum), activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn_128_long.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1000, 100)    959700      input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 999, 64)      12864       embedding_3[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 997, 64)      25664       embedding_3[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 996, 64)      32064       embedding_3[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_31 (Global (None, 64)           0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (Global (None, 64)           0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_33 (Global (None, 64)           0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 192)          0           global_max_pooling1d_31[0][0]    \n",
      "                                                                 global_max_pooling1d_32[0][0]    \n",
      "                                                                 global_max_pooling1d_33[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 64)           12352       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 3)            195         dense_21[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,042,839\n",
      "Trainable params: 1,042,839\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(64, 2, activation='relu')(embedded_sequences)\n",
    "l_pool1 = GlobalMaxPooling1D()(l_cov1)\n",
    "l_cov2 = Conv1D(64, 4, activation='relu')(embedded_sequences)\n",
    "l_pool2 = GlobalMaxPooling1D()(l_cov2)\n",
    "l_cov3 = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "l_pool3 = GlobalMaxPooling1D()(l_cov3)  # global max pooling\n",
    "l_flat = Concatenate()([l_pool1,l_pool2,l_pool3])\n",
    "l_dense = Dense(64, activation='relu')(l_flat)\n",
    "preds = Dense(len(macronum), activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn_128_long.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1032 samples, validate on 258 samples\n",
      "Epoch 1/200\n",
      "1032/1032 [==============================] - 25s 25ms/step - loss: 1.3422 - acc: 0.3750 - val_loss: 1.0330 - val_acc: 0.4496\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.44961, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 2/200\n",
      "1032/1032 [==============================] - 25s 25ms/step - loss: 0.9881 - acc: 0.5291 - val_loss: 1.4560 - val_acc: 0.3372\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.44961\n",
      "Epoch 3/200\n",
      "1032/1032 [==============================] - 26s 25ms/step - loss: 0.8786 - acc: 0.6047 - val_loss: 1.7011 - val_acc: 0.3450\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.44961\n",
      "Epoch 4/200\n",
      "1032/1032 [==============================] - 26s 26ms/step - loss: 0.8320 - acc: 0.6647 - val_loss: 1.3439 - val_acc: 0.4070\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.44961\n",
      "Epoch 5/200\n",
      "1032/1032 [==============================] - 26s 25ms/step - loss: 0.7312 - acc: 0.7161 - val_loss: 1.1092 - val_acc: 0.4535\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.44961 to 0.45349, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 6/200\n",
      "1032/1032 [==============================] - 26s 26ms/step - loss: 0.6865 - acc: 0.7122 - val_loss: 1.0423 - val_acc: 0.4961\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.45349 to 0.49612, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 7/200\n",
      "1032/1032 [==============================] - 26s 25ms/step - loss: 0.6171 - acc: 0.7442 - val_loss: 0.8221 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.49612 to 0.61628, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 8/200\n",
      "1032/1032 [==============================] - 25s 24ms/step - loss: 0.5048 - acc: 0.8314 - val_loss: 1.0011 - val_acc: 0.5659\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.61628\n",
      "Epoch 9/200\n",
      "1032/1032 [==============================] - 25s 25ms/step - loss: 0.4654 - acc: 0.8556 - val_loss: 0.8013 - val_acc: 0.6202\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.61628 to 0.62016, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 10/200\n",
      "1032/1032 [==============================] - 27s 26ms/step - loss: 0.4468 - acc: 0.8304 - val_loss: 0.8092 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.62016\n",
      "Epoch 11/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.3708 - acc: 0.8857 - val_loss: 1.0048 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.62016\n",
      "Epoch 12/200\n",
      "1032/1032 [==============================] - 23s 23ms/step - loss: 0.2797 - acc: 0.9215 - val_loss: 1.4681 - val_acc: 0.5388\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.62016\n",
      "Epoch 13/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.2887 - acc: 0.9099 - val_loss: 1.1677 - val_acc: 0.5116\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.62016\n",
      "Epoch 14/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.2547 - acc: 0.9196 - val_loss: 1.4948 - val_acc: 0.5271\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.62016\n",
      "Epoch 15/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.2522 - acc: 0.9002 - val_loss: 0.8431 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.62016\n",
      "Epoch 16/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1774 - acc: 0.9506 - val_loss: 0.9026 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.62016\n",
      "Epoch 17/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1863 - acc: 0.9360 - val_loss: 1.9355 - val_acc: 0.4302\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.62016\n",
      "Epoch 18/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1621 - acc: 0.9448 - val_loss: 1.0681 - val_acc: 0.5659\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.62016\n",
      "Epoch 19/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1912 - acc: 0.9360 - val_loss: 0.9824 - val_acc: 0.6163\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.62016\n",
      "Epoch 20/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1050 - acc: 0.9651 - val_loss: 0.9388 - val_acc: 0.6279\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.62016 to 0.62791, saving model to model_cnn_128_long.hdf5\n",
      "Epoch 21/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1633 - acc: 0.9457 - val_loss: 1.3207 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.62791\n",
      "Epoch 22/200\n",
      "1032/1032 [==============================] - 23s 23ms/step - loss: 0.0977 - acc: 0.9680 - val_loss: 1.0035 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.62791\n",
      "Epoch 23/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1355 - acc: 0.9574 - val_loss: 1.0291 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.62791\n",
      "Epoch 24/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1076 - acc: 0.9641 - val_loss: 1.0914 - val_acc: 0.6085\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.62791\n",
      "Epoch 25/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1291 - acc: 0.9506 - val_loss: 0.9720 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.62791\n",
      "Epoch 26/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1520 - acc: 0.9399 - val_loss: 1.0399 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.62791\n",
      "Epoch 27/200\n",
      "1032/1032 [==============================] - 23s 23ms/step - loss: 0.0868 - acc: 0.9700 - val_loss: 0.9770 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.62791\n",
      "Epoch 28/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1322 - acc: 0.9516 - val_loss: 1.0016 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.62791\n",
      "Epoch 29/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0972 - acc: 0.9651 - val_loss: 2.4597 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.62791\n",
      "Epoch 30/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1177 - acc: 0.9612 - val_loss: 1.0504 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.62791\n",
      "Epoch 31/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1506 - acc: 0.9428 - val_loss: 1.1067 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.62791\n",
      "Epoch 32/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0785 - acc: 0.9661 - val_loss: 1.9397 - val_acc: 0.4806\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.62791\n",
      "Epoch 33/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1075 - acc: 0.9574 - val_loss: 1.0399 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.62791\n",
      "Epoch 34/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0942 - acc: 0.9671 - val_loss: 1.0103 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.62791\n",
      "Epoch 35/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1064 - acc: 0.9661 - val_loss: 1.5187 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.62791\n",
      "Epoch 36/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0858 - acc: 0.9641 - val_loss: 1.0165 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.62791\n",
      "Epoch 37/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0970 - acc: 0.9661 - val_loss: 1.0374 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.62791\n",
      "Epoch 38/200\n",
      "1032/1032 [==============================] - 23s 23ms/step - loss: 0.0955 - acc: 0.9651 - val_loss: 1.3772 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.62791\n",
      "Epoch 39/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0993 - acc: 0.9680 - val_loss: 1.2368 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.62791\n",
      "Epoch 40/200\n",
      "1032/1032 [==============================] - 23s 23ms/step - loss: 0.0808 - acc: 0.9680 - val_loss: 1.5698 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.62791\n",
      "Epoch 41/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.1096 - acc: 0.9671 - val_loss: 1.1547 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.62791\n",
      "Epoch 42/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0925 - acc: 0.9651 - val_loss: 1.1058 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.62791\n",
      "Epoch 43/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0929 - acc: 0.9641 - val_loss: 1.1314 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.62791\n",
      "Epoch 44/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0805 - acc: 0.9690 - val_loss: 1.2805 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.62791\n",
      "Epoch 45/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0949 - acc: 0.9641 - val_loss: 1.3712 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.62791\n",
      "Epoch 46/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0916 - acc: 0.9651 - val_loss: 1.3102 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.62791\n",
      "Epoch 47/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0872 - acc: 0.9661 - val_loss: 1.1686 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.62791\n",
      "Epoch 48/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0929 - acc: 0.9651 - val_loss: 1.0554 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.62791\n",
      "Epoch 49/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0886 - acc: 0.9641 - val_loss: 1.1330 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.62791\n",
      "Epoch 50/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0849 - acc: 0.9651 - val_loss: 1.1967 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.62791\n",
      "Epoch 51/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0849 - acc: 0.9661 - val_loss: 1.3327 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.62791\n",
      "Epoch 52/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0886 - acc: 0.9661 - val_loss: 2.1141 - val_acc: 0.5543\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.62791\n",
      "Epoch 53/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0884 - acc: 0.9661 - val_loss: 1.3023 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.62791\n",
      "Epoch 54/200\n",
      "1032/1032 [==============================] - 23s 23ms/step - loss: 0.0858 - acc: 0.9622 - val_loss: 1.1676 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.62791\n",
      "Epoch 55/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0820 - acc: 0.9651 - val_loss: 1.1980 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.62791\n",
      "Epoch 56/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0814 - acc: 0.9671 - val_loss: 2.1393 - val_acc: 0.5504\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.62791\n",
      "Epoch 57/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0931 - acc: 0.9661 - val_loss: 1.5324 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.62791\n",
      "Epoch 58/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0816 - acc: 0.9690 - val_loss: 1.2466 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.62791\n",
      "Epoch 59/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0831 - acc: 0.9680 - val_loss: 1.2231 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.62791\n",
      "Epoch 60/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0769 - acc: 0.9680 - val_loss: 1.6468 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.62791\n",
      "Epoch 61/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0827 - acc: 0.9671 - val_loss: 1.3818 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.62791\n",
      "Epoch 62/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0765 - acc: 0.9680 - val_loss: 1.3465 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.62791\n",
      "Epoch 63/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0819 - acc: 0.9671 - val_loss: 1.3375 - val_acc: 0.5659\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.62791\n",
      "Epoch 64/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0825 - acc: 0.9632 - val_loss: 1.4085 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.62791\n",
      "Epoch 65/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0819 - acc: 0.9632 - val_loss: 1.4857 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.62791\n",
      "Epoch 66/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0794 - acc: 0.9661 - val_loss: 1.6679 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.62791\n",
      "Epoch 67/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0775 - acc: 0.9680 - val_loss: 1.6505 - val_acc: 0.5775\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.62791\n",
      "Epoch 68/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0819 - acc: 0.9671 - val_loss: 1.6240 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.62791\n",
      "Epoch 69/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0764 - acc: 0.9700 - val_loss: 1.2207 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.62791\n",
      "Epoch 70/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0789 - acc: 0.9632 - val_loss: 1.2580 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.62791\n",
      "Epoch 71/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0766 - acc: 0.9671 - val_loss: 1.4516 - val_acc: 0.5736\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.62791\n",
      "Epoch 72/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0786 - acc: 0.9651 - val_loss: 1.4108 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.62791\n",
      "Epoch 73/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0793 - acc: 0.9700 - val_loss: 2.0867 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.62791\n",
      "Epoch 74/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0763 - acc: 0.9622 - val_loss: 1.3235 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.62791\n",
      "Epoch 75/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0696 - acc: 0.9690 - val_loss: 1.3157 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.62791\n",
      "Epoch 76/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0694 - acc: 0.9671 - val_loss: 1.7528 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.62791\n",
      "Epoch 77/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0771 - acc: 0.9680 - val_loss: 1.6703 - val_acc: 0.5620\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.62791\n",
      "Epoch 78/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0771 - acc: 0.9651 - val_loss: 1.2821 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.62791\n",
      "Epoch 79/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0700 - acc: 0.9671 - val_loss: 1.6833 - val_acc: 0.5659\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.62791\n",
      "Epoch 80/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0705 - acc: 0.9690 - val_loss: 1.6968 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.62791\n",
      "Epoch 81/200\n",
      "1032/1032 [==============================] - 23s 23ms/step - loss: 0.0729 - acc: 0.9680 - val_loss: 1.4454 - val_acc: 0.5930\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.62791\n",
      "Epoch 82/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0709 - acc: 0.9700 - val_loss: 1.6072 - val_acc: 0.5465\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.62791\n",
      "Epoch 83/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0682 - acc: 0.9651 - val_loss: 1.4929 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.62791\n",
      "Epoch 84/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0709 - acc: 0.9671 - val_loss: 1.3818 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.62791\n",
      "Epoch 85/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0652 - acc: 0.9700 - val_loss: 1.5489 - val_acc: 0.5969\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.62791\n",
      "Epoch 86/200\n",
      "1032/1032 [==============================] - 23s 22ms/step - loss: 0.0683 - acc: 0.9690 - val_loss: 1.4071 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.62791\n",
      "Epoch 87/200\n",
      "1032/1032 [==============================] - 24s 23ms/step - loss: 0.0682 - acc: 0.9661 - val_loss: 1.4424 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.62791\n",
      "Epoch 88/200\n",
      " 640/1032 [=================>............] - ETA: 8s - loss: 0.0746 - acc: 0.9625 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-27b29b870645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/tf/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=200, batch_size=64,callbacks=[cp])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM and CNN for sequence classification in the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 1000, 64)          3200000   \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 1000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 333, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 3,253,763\n",
      "Trainable params: 3,253,763\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 50000\n",
    "#(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = MAX_SEQUENCE_LENGTH\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(x_val, maxlen=max_review_length)\n",
    "# create the model\n",
    "\n",
    "embedding_vecor_length = 64\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "print(model.summary())\n",
    "cp=ModelCheckpoint('model_cnn_lstm_128_long.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n",
    "#model.fit(X_train, y_train, validation_data = (X_test, y_val), epochs=50, batch_size=64, callbacks=[cp])\n",
    "# Final evaluation of the model\n",
    "#scores = model.evaluate(X_test, y_val, verbose=0)\n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1032 samples, validate on 258 samples\n",
      "Epoch 1/20\n",
      "1032/1032 [==============================] - 51s 49ms/step - loss: 1.0973 - acc: 0.3508 - val_loss: 1.0959 - val_acc: 0.3101\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.31008, saving model to model_cnn_lstm_128_long.hdf5\n",
      "Epoch 2/20\n",
      "1032/1032 [==============================] - 49s 48ms/step - loss: 1.0883 - acc: 0.3643 - val_loss: 1.0944 - val_acc: 0.3101\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.31008\n",
      "Epoch 3/20\n",
      "1032/1032 [==============================] - 41s 40ms/step - loss: 1.0723 - acc: 0.3576 - val_loss: 1.0767 - val_acc: 0.4302\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.31008 to 0.43023, saving model to model_cnn_lstm_128_long.hdf5\n",
      "Epoch 4/20\n",
      "1032/1032 [==============================] - 65s 63ms/step - loss: 1.0271 - acc: 0.5678 - val_loss: 1.1096 - val_acc: 0.3798\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.43023\n",
      "Epoch 5/20\n",
      "1032/1032 [==============================] - 73s 71ms/step - loss: 1.0391 - acc: 0.4622 - val_loss: 1.0287 - val_acc: 0.5233\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.43023 to 0.52326, saving model to model_cnn_lstm_128_long.hdf5\n",
      "Epoch 6/20\n",
      "1032/1032 [==============================] - 75s 73ms/step - loss: 0.9670 - acc: 0.6570 - val_loss: 0.9868 - val_acc: 0.5155\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.52326\n",
      "Epoch 7/20\n",
      "1032/1032 [==============================] - 78s 75ms/step - loss: 0.9025 - acc: 0.6890 - val_loss: 1.2623 - val_acc: 0.3837\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.52326\n",
      "Epoch 8/20\n",
      "1032/1032 [==============================] - 63s 61ms/step - loss: 1.0396 - acc: 0.5756 - val_loss: 0.9629 - val_acc: 0.5620\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.52326 to 0.56202, saving model to model_cnn_lstm_128_long.hdf5\n",
      "Epoch 9/20\n",
      "1032/1032 [==============================] - 51s 49ms/step - loss: 0.8438 - acc: 0.7607 - val_loss: 0.9615 - val_acc: 0.5233\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.56202\n",
      "Epoch 10/20\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 0.8230 - acc: 0.6657 - val_loss: 1.0296 - val_acc: 0.4651\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.56202\n",
      "Epoch 11/20\n",
      "1032/1032 [==============================] - 52s 50ms/step - loss: 0.8226 - acc: 0.7141 - val_loss: 1.1039 - val_acc: 0.4419\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.56202\n",
      "Epoch 12/20\n",
      "1032/1032 [==============================] - 51s 50ms/step - loss: 0.8288 - acc: 0.6686 - val_loss: 0.9210 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.56202 to 0.58140, saving model to model_cnn_lstm_128_long.hdf5\n",
      "Epoch 13/20\n",
      "1032/1032 [==============================] - 50s 49ms/step - loss: 0.7140 - acc: 0.7926 - val_loss: 0.9115 - val_acc: 0.6202\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.58140 to 0.62016, saving model to model_cnn_lstm_128_long.hdf5\n",
      "Epoch 14/20\n",
      "1032/1032 [==============================] - 43s 41ms/step - loss: 0.7120 - acc: 0.8023 - val_loss: 0.9239 - val_acc: 0.6047\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.62016\n",
      "Epoch 15/20\n",
      "1032/1032 [==============================] - 56s 54ms/step - loss: 0.6328 - acc: 0.8895 - val_loss: 1.1838 - val_acc: 0.4380\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.62016\n",
      "Epoch 16/20\n",
      "1032/1032 [==============================] - 46s 45ms/step - loss: 0.7993 - acc: 0.6609 - val_loss: 0.8898 - val_acc: 0.6240\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.62016 to 0.62403, saving model to model_cnn_lstm_128_long.hdf5\n",
      "Epoch 17/20\n",
      "1032/1032 [==============================] - 47s 46ms/step - loss: 0.5471 - acc: 0.9031 - val_loss: 1.0042 - val_acc: 0.6008\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.62403\n",
      "Epoch 18/20\n",
      "1032/1032 [==============================] - 44s 42ms/step - loss: 0.6076 - acc: 0.8043 - val_loss: 0.9457 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.62403\n",
      "Epoch 19/20\n",
      "1032/1032 [==============================] - 44s 42ms/step - loss: 0.5402 - acc: 0.8769 - val_loss: 0.9738 - val_acc: 0.5659\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.62403\n",
      "Epoch 20/20\n",
      "1032/1032 [==============================] - 45s 44ms/step - loss: 0.5411 - acc: 0.8702 - val_loss: 1.0138 - val_acc: 0.5543\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.62403\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=20, batch_size=512,callbacks=[cp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### With features\n",
    "#CNN\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "audio_feat = Input(shape=(n_features,), dtype='floatoat32')###\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 2, activation='relu')(embedded_sequences)\n",
    "l_pool1 = GlobalMaxPooling1D()(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool2 = GlobalMaxPooling1D()(l_cov2)\n",
    "l_cov3 = Conv1D(128, 10, activation='relu')(embedded_sequences)\n",
    "l_pool3 = GlobalMaxPooling1D()(l_cov3)  # global max pooling\n",
    "l_flat = Concatenate()([l_pool1,l_pool2,l_pool3,audio_feat])\n",
    "l_flat = Dropout(rate=0.25)(l_flat)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "l_dense = Dropout(rate=0.25)(l_dense)\n",
    "l_dense1 = Dense(128, activation='relu')(l_dense)\n",
    "l_dense1 = Dropout(rate=0.25)(l_dense1)\n",
    "\n",
    "\n",
    "preds = Dense(len(macronum), activation='softmax')(l_dense)\n",
    "\n",
    "model = Model([sequence_input, audio_feat], preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn_experimental_2.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 100)    1299400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 999, 128)     25728       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 996, 128)     64128       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 991, 128)     128128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          49280       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            516         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,583,692\n",
      "Trainable params: 1,583,692\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 2, activation='relu')(embedded_sequences)\n",
    "l_pool1 = GlobalMaxPooling1D()(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool2 = GlobalMaxPooling1D()(l_cov2)\n",
    "l_cov3 = Conv1D(128, 10, activation='relu')(embedded_sequences)\n",
    "l_pool3 = GlobalMaxPooling1D()(l_cov3)  # global max pooling\n",
    "l_flat = Concatenate()([l_pool1,l_pool2,l_pool3])\n",
    "l_flat = Dropout(rate=0.25)(l_flat)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "l_dense = Dropout(rate=0.25)(l_dense)\n",
    "l_dense1 = Dense(128, activation='relu')(l_dense)\n",
    "l_dense1 = Dropout(rate=0.25)(l_dense1)\n",
    "\n",
    "\n",
    "preds = Dense(len(macronum), activation='softmax')(l_dense1)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn_experimental_long.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network 2\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         1303700   \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,548,952\n",
      "Trainable params: 1,548,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(len(macronum), activation='softmax')(l_dense)\n",
    "\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network 2\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn2.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         1299400   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 996, 64)           32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 199, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 195, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 39, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 35, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 7, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                28736     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 1,401,548\n",
      "Trainable params: 1,401,548\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(64, 5, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "l_cov3 = Conv1D(64, 5, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(5)(l_cov3)  # global max pooling\n",
    "l_flat = Flatten()(l_pool3)\n",
    "l_dense = Dense(64, activation='relu')(l_flat)\n",
    "preds = Dense(len(macronum), activation='softmax')(l_dense)\n",
    "\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn_jatana.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1883 samples, validate on 470 samples\n",
      "Epoch 1/50\n",
      "1883/1883 [==============================] - 24s 13ms/step - loss: 0.0726 - acc: 0.9671 - val_loss: 1.4594 - val_acc: 0.6894\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.71277\n",
      "Epoch 2/50\n",
      "1883/1883 [==============================] - 24s 13ms/step - loss: 0.0652 - acc: 0.9687 - val_loss: 1.5721 - val_acc: 0.6723\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.71277\n",
      "Epoch 3/50\n",
      " 832/1883 [============>.................] - ETA: 12s - loss: 0.0829 - acc: 0.9579"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=50, batch_size=64,callbacks=[cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.674468085106383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.18      0.29        17\n",
      "           1       0.74      0.80      0.77       210\n",
      "           2       0.56      0.39      0.46        80\n",
      "           3       0.62      0.71      0.66       163\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       470\n",
      "   macro avg       0.67      0.52      0.54       470\n",
      "weighted avg       0.67      0.67      0.66       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_val)\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "y_test = np.argmax(y_val,axis=1)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model\n",
    "model2.load_weights(\"model_cnn_128_long.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7191489361702128\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.18      0.31        11\n",
      "           1       0.72      0.90      0.80       198\n",
      "           2       0.77      0.43      0.55        95\n",
      "           3       0.70      0.70      0.70       166\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       470\n",
      "   macro avg       0.80      0.55      0.59       470\n",
      "weighted avg       0.73      0.72      0.70       470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_val)\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "y_test = np.argmax(y_val,axis=1)\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98 0.81 0.86 0.79]\n"
     ]
    }
   ],
   "source": [
    "print(ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.26 0.03 0.16]\n"
     ]
    }
   ],
   "source": [
    "print (FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82 0.1  0.57 0.3 ]\n"
     ]
    }
   ],
   "source": [
    "print(FNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    198\n",
       "3    166\n",
       "2     95\n",
       "0     11\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame(y_test, columns=['label'])\n",
    "test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 1 1 1 1 3 3 1 3 3 3 2 3 3 3 3 3 1 3 1 1 3 1 3 3 2 1 1 3 1 1 2 3 1 3\n",
      " 2 1 3 2 3 3 1 3 3 1 1 2 2 1 1 2 3 1 2 1 1 1 3 3 2 2 2 1 3 0 2 1 1 1 1 2 1\n",
      " 1 3 3 1 3 3 2 3 3 1 1 3 1 3 1 1 3 1 2 2 2 1 2 1 2 2 1 1 1 3 1 2 3 3 3 2 1\n",
      " 3 1 3 1 1 1 3 1 1 3 3 1 3 1 2 2 1 3 1 3 1 1 1 3 3 1 1 1 3 1 2 1 2 0 3 1 1\n",
      " 3 1 1 2 3 3 1 3 2 1 3 2 1 0 3 1 2 1 1 2 1 1 0 2 3 2 3 3 1 3 1 3 1 2 1 1 2\n",
      " 2 1 1 3 3 3 0 1 3 3 2 3 2 1 1 3 1 2 3 2 1 1 1 1 1 1 1 3 1 3 1 1 3 3 1 3 3\n",
      " 3 1 2 1 2 3 3 1 1 3 3 1 2 2 2 1 1 3 0 1 1 3 1 1 2 2 1 2 1 1 3 3 3 1 1 3 2\n",
      " 1 3 1 2 3 3 2 3 1 2 3 1 3 1 1 1 1 1 1 2 1 3 1 1 3 1 2 2 1 3 3 3 1 2 1 3 3\n",
      " 1 3 1 1 1 1 3 1 3 1 2 3 1 3 3 3 2 1 2 2 1 1 2 1 1 2 1 1 0 2 1 3 2 1 2 1 3\n",
      " 1 3 3 2 2 1 1 3 1 3 3 1 2 3 2 3 1 2 1 3 2 1 1 3 3 0 3 2 3 3 3 3 3 3 1 1 1\n",
      " 3 1 1 3 2 1 3 1 3 1 1 2 3 2 1 2 1 2 3 3 2 3 1 1 1 1 1 3 3 2 2 1 1 2 3 3 2\n",
      " 3 1 3 2 2 2 1 3 1 3 3 1 3 1 1 1 3 1 3 3 1 1 1 3 3 1 3 1 3 3 0 1 3 1 3 1 1\n",
      " 3 1 1 3 2 3 3 2 0 0 2 2 1 1 3 2 1 1 2 3 3 2 2 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 1 1 1 1 1 1 1 1 3 2 1 2 1 2 3 3 1 1 3 1 1 3 1 1 3 1 1 1 3 1 1 1 3 1 3\n",
      " 2 1 3 2 3 3 1 3 3 1 1 2 3 1 1 2 3 1 3 1 1 1 3 3 3 2 1 1 1 3 3 1 1 1 1 1 1\n",
      " 1 1 3 1 3 3 2 1 3 1 1 1 1 3 1 1 3 1 2 3 1 1 3 1 2 1 1 3 1 3 1 3 3 3 2 3 1\n",
      " 3 1 1 1 1 1 3 1 1 3 3 1 3 1 2 3 1 1 1 3 1 1 1 3 1 1 1 1 3 1 3 1 1 1 1 1 1\n",
      " 1 1 1 2 3 1 3 1 2 1 3 1 1 3 3 1 1 1 1 2 1 1 1 2 1 3 3 3 3 3 3 3 1 2 1 1 1\n",
      " 3 1 1 3 3 3 1 1 3 3 1 3 3 1 1 3 1 2 3 2 3 1 1 2 1 1 2 3 1 3 1 1 3 3 1 3 3\n",
      " 3 1 1 1 2 3 3 1 1 3 3 1 3 1 3 1 3 1 3 1 1 1 1 1 1 2 1 3 1 1 3 3 1 1 1 3 2\n",
      " 1 2 1 2 3 1 2 2 1 2 3 3 3 1 1 1 1 1 1 3 1 3 1 1 3 1 2 1 1 3 3 3 1 2 1 3 3\n",
      " 1 3 1 1 1 3 3 1 1 2 3 1 1 3 3 3 2 1 2 2 1 1 2 1 1 1 1 1 1 3 1 2 3 1 1 1 3\n",
      " 3 3 2 2 1 1 3 3 1 1 2 1 3 3 2 1 1 1 1 3 2 1 1 1 3 3 3 2 1 3 3 3 1 3 1 1 1\n",
      " 3 1 3 1 1 1 3 1 3 1 1 3 3 3 1 3 1 2 3 3 1 3 1 1 1 1 1 3 1 3 3 1 1 2 3 3 3\n",
      " 3 1 1 2 1 2 1 3 1 1 3 1 3 3 3 3 3 1 1 3 1 1 1 3 3 1 1 1 3 1 0 1 3 1 1 1 1\n",
      " 1 1 2 3 1 1 3 3 3 0 1 1 1 1 3 3 1 3 2 3 3 2 2 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6404494382022472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.62      0.51        16\n",
      "           1       0.77      0.72      0.74       239\n",
      "           2       0.59      0.22      0.32        99\n",
      "           3       0.55      0.77      0.64       180\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       534\n",
      "   macro avg       0.59      0.58      0.56       534\n",
      "weighted avg       0.65      0.64      0.62       534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_val)\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "y_test = np.argmax(y_val,axis=1)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20224719101123595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        17\n",
      "           1       0.00      0.00      0.00       223\n",
      "           2       0.20      0.99      0.34       109\n",
      "           3       0.00      0.00      0.00       185\n",
      "\n",
      "   micro avg       0.20      0.20      0.20       534\n",
      "   macro avg       0.05      0.25      0.08       534\n",
      "weighted avg       0.04      0.20      0.07       534\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/revanth/tf/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_val)\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "y_test = np.argmax(y_val,axis=1)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-2aa57a1383be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_plot.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_model' is not defined"
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :CNN',fontsize=16)\n",
    "fig1.savefig('loss_cnn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig2=plt.figure()\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves : CNN',fontsize=16)\n",
    "fig2.savefig('accuracy_cnn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 100)    1303700     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 999, 128)     25728       embedding_1[10][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 996, 128)     64128       embedding_1[10][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 991, 128)     128128      embedding_1[10][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 384)          0           global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 128)          49280       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 4)            516         dense_22[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,571,480\n",
      "Trainable params: 1,571,480\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "l_cov1= Conv1D(128, 2, activation='relu')(embedded_sequences)\n",
    "l_pool1 = GlobalMaxPooling1D()(l_cov1)\n",
    "l_cov2 = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "l_pool2 = GlobalMaxPooling1D()(l_cov2)\n",
    "l_cov3 = Conv1D(128, 10, activation='relu')(embedded_sequences)\n",
    "l_pool3 = GlobalMaxPooling1D()(l_cov3)  # global max pooling\n",
    "l_flat = Concatenate()([l_pool1,l_pool2,l_pool3])\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "\n",
    "\n",
    "preds = Dense(len(macronum), activation='softmax')(l_dense)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()\n",
    "cp=ModelCheckpoint('model_cnn_experimental_long.hdf5',monitor='val_acc',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
